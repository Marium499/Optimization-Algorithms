{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Binary Classification with Support Vector Sampling and Adaptive SGD\n",
    "\n",
    "This notebook builds on earlier work for **binary classification using Stochastic Gradient Descent (SGD)**\n",
    "on the Fashion MNIST dataset. It introduces a mechanism to **sample support vectors** from the test set,\n",
    "adds a regularized offset term to images, and uses a custom non-convex logistic loss with multiple adaptive\n",
    "step size strategies.\n",
    "\n",
    "**Goals:**\n",
    "- Prepare a binary classification dataset for two classes\n",
    "- Run SGD using different step size policies (diminishing, Polyak)\n",
    "- Evaluate performance using error rate and support vector sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and SGD Training\n",
    "\n",
    "**`prepare_fashion_mnist(class1, class2)`** loads and filters two target classes from Fashion MNIST,\n",
    "then appends a regularization term to each image. A subset of 500 points (250 per class) is sampled\n",
    "as *support vectors* from the test set and returned separately for potential margin analysis or visualization.\n",
    "\n",
    "**`sgd_mlogistic(trainXr, gamma, stepsizefunc, numit)`** performs SGD with a given step size strategy,\n",
    "maintaining and averaging intermediate weights to stabilize training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_fashion_mnist(class1, class2) -> x_train, y_train, x_test, y_test\n",
    "# class1, class2: 0~9\n",
    "# read fashion mnist from file\n",
    "# get trainX, trainY, testX and testY from file that corresponds to class1 and class2\n",
    "# find q = average value of all pixels across all images in trainX\n",
    "# append q to every image in trainX and testX as the 785th pixel/element - offset term - serves as regularization and bias\n",
    "# flip sign of class1 in trainY and testY\n",
    "# return x_train, y_train, x_test, y_test\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def prepare_fashion_mnist(class1, class2):\n",
    "\n",
    "    # load data\n",
    "    data = loadmat('fashion_mnist.mat')\n",
    "    trainX = data['trainX_uint8']\n",
    "    trainY = data['trainY_uint8']\n",
    "    testX = data['testX_uint8']\n",
    "    testY = data['testY_uint8']\n",
    "\n",
    "    # filter out class1 and class2\n",
    "\n",
    "    train_indices = np.where((trainY == class1) | (trainY == class2))\n",
    "    trainX = trainX[:,train_indices[0]]\n",
    "    trainY = trainY[train_indices[0]]\n",
    "\n",
    "    class1_indices = np.where((testY == class1))\n",
    "    class2_indices = np.where((testY == class2))\n",
    "    class1_random_indices = np.random.choice(class1_indices[0], 250, replace=False)\n",
    "    class2_random_indices = np.random.choice(class2_indices[0], 250, replace=False)\n",
    "    svs_indices = np.concatenate((class1_random_indices, class2_random_indices))\n",
    "\n",
    "\n",
    "    \n",
    "    svsX = testX[:,svs_indices]\n",
    "    svsY = testY[svs_indices]\n",
    "    testX = np.delete(testX, svs_indices, axis=1)\n",
    "    testY = np.delete(testY, svs_indices, axis=0)\n",
    "\n",
    "    test_indices = np.where((testY == class1) | (testY == class2))\n",
    "    testX = testX[:,test_indices[0]]\n",
    "    testY = testY[test_indices[0]]\n",
    "    \n",
    "\n",
    "    #convert to float\n",
    "    trainX = trainX.astype(float)\n",
    "    testX = testX.astype(float)\n",
    "\n",
    "\n",
    "    # find q\n",
    "    q = np.mean(trainX)\n",
    "\n",
    "    # append q to every image in trainX and testX as the 785th pixel/element\n",
    "    trainX = np.append(trainX, q * np.ones((1, trainX.shape[1])), axis=0)\n",
    "    testX = np.append(testX, q * np.ones((1, testX.shape[1])), axis=0)\n",
    "    svsX = np.append(svsX, q * np.ones((1, svsX.shape[1])), axis=0)\n",
    "\n",
    "    # flip sign of class1 in trainX and testX\n",
    "    class1_indices = np.where((trainY == class1))\n",
    "    trainX[:,class1_indices[0]] = -trainX[:,class1_indices[0]]\n",
    "\n",
    "    class1_indices = np.where((testY == class1))\n",
    "    testX[:,class1_indices[0]] = -testX[:,class1_indices[0]]\n",
    "\n",
    "    class1_indices = np.where((svsY == class1))\n",
    "    svsX[:,class1_indices[0]] = -svsX[:,class1_indices[0]]\n",
    "\n",
    "    return trainX.T, trainY.T, testX.T, testY.T, svsX.T, svsY.T\n",
    "\n",
    "\n",
    "def sgd_mlogistic(trainXr, gamma, stepsizefunc, numit):\n",
    "\n",
    "    #initialize weights\n",
    "    x = np.zeros((trainXr.shape[1], 1))\n",
    "    # random.seed(42)\n",
    "    x_list = []\n",
    "    for k in range(numit):\n",
    "        \n",
    "        # choose random index\n",
    "        i = random.randint(0, trainXr.shape[0]-1)\n",
    "        ai = trainXr[i, :].reshape(-1,1)\n",
    "        alpha = stepsizefunc(ai, x, k)\n",
    "        grad = gradient_f(x, ai, gamma)\n",
    "        x = x - alpha*grad\n",
    "        x_list.append(x)\n",
    "\n",
    "    return np.mean(x_list, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient, Loss, and Learning Rate Functions\n",
    "\n",
    "This section defines:\n",
    "- `gradient_f`: custom logistic loss gradient (non-convex)\n",
    "- `f_x`: loss function combining data fit and regularization\n",
    "- `phi_1`, `phi_2`, `phi_3`: fixed and diminishing step size policies\n",
    "- `stoch_polyak`: Polyak-style adaptive step size that uses a local estimate of the optimal value\n",
    "- `calculate_error`: returns classification error on a given dataset\n",
    "- Utility methods for Newton's method and approximating minima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER FUNCTIONS\n",
    "import math\n",
    "\n",
    "def P(c):\n",
    "    return lambda t: 1 - 4 * t + 4 * (t**2) - 4 * c * (t**3) + 4 * c * (t**4)\n",
    "\n",
    "\n",
    "def P_prime(c):\n",
    "    return lambda t: -4 + 8 * t - 12 * c * (t**2) + 16 * c * (t**3)\n",
    "\n",
    "\n",
    "def newton_method_on_P(c):\n",
    "    initial_guess = min(0.5, (4 * c) ** (-1 / 3))\n",
    "    last_x = initial_guess\n",
    "\n",
    "    for i in range(20):\n",
    "        x = last_x - P(c)(last_x) / P_prime(c)(last_x)\n",
    "        last_x = x\n",
    "    return last_x\n",
    "\n",
    "def gradient_f(x, a, gamma):\n",
    "    # Compute the gradient of f with respect to x for a single data point a\n",
    "    gradient = (0.5 + 0.5 * np.dot(a.T, x) / np.sqrt(1 + np.dot(a.T, x)**2)) * a\n",
    "    gradient += gamma * x  # Add regularization term\n",
    "    return gradient\n",
    "\n",
    "def f_x(x, a, gamma):\n",
    "    reg_term = (gamma / 2) * np.linalg.norm(x)**2\n",
    "\n",
    "    t = np.dot(a.T, x)\n",
    "    psi_t = psi(t)\n",
    "    return psi_t + reg_term\n",
    "\n",
    "\n",
    "def find_min_of_f(a_norm_squared, gamma):\n",
    "    c = (a_norm_squared**2) / (gamma**2)\n",
    "    t_star = newton_method_on_P(c)\n",
    "    theta_star = t_star / gamma\n",
    "    psi = lambda t: t / 2 + math.sqrt(1 + t**2) / 2\n",
    "    return theta_star, psi(-a_norm_squared * theta_star) + (\n",
    "        gamma / 2\n",
    "    ) * a_norm_squared * (theta_star**2)\n",
    "\n",
    "def psi(t):\n",
    "    return t / 2 + np.sqrt(1 + t**2) / 2\n",
    "\n",
    "def phi_1(a, x, k, gamma):\n",
    "    L = 4*gamma**2\n",
    "\n",
    "    return (gamma*1e-2)/L\n",
    "\n",
    "def phi_2(a, x, k, gamma):\n",
    "    L = 4*gamma**2\n",
    "\n",
    "    return (gamma*1e-5)/L\n",
    "\n",
    "def phi_3(a, x, k, gamma):\n",
    "    L = 4*gamma**2\n",
    "    m = gamma\n",
    "    c = L/(m**2)\n",
    "\n",
    "    return 1/(m*(c + 0.75*k))\n",
    "\n",
    "def stoch_polyak(a, x, gamma, gammab=1e4):\n",
    "    \n",
    "    _, f_min = find_min_of_f(np.linalg.norm(a)**2, gamma)\n",
    "    f_current = f_x(x, a, gamma)\n",
    "    squared_norm_grad = np.linalg.norm(gradient_f(x, a, gamma))**2\n",
    "    alpha = min(gammab, 2 * (f_current - f_min) / squared_norm_grad)\n",
    "    # print(alpha)\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "def calculate_error(dataXr, x):\n",
    "    predictions = np.dot(dataXr, x)\n",
    "    # Determine misclassified points (nonnegative means misclassified)\n",
    "    misclassified = predictions[predictions >= 0]\n",
    "    error_rate = len(misclassified)/len(predictions)\n",
    "\n",
    "    return error_rate, misclassified\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def display_misclassified_image(dataXr, dataYr, x,actual_labels, class1, class2):\n",
    "    \"\"\"\n",
    "    Display a misclassified image with a label showing the actual and misclassified classes.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataXr: The test dataset (or train dataset), including the augmented pixel for offset.\n",
    "    - dataYr: The labels of the test dataset.\n",
    "    - x: The model weights from sgd_mlogistic.\n",
    "    - actual_labels: Original labels for class1 and class2 (e.g., 0 for T-shirt, 9 for ankle-boot).\n",
    "    - class1, class2: The integer class labels (for binary classification).\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    predictions = np.dot(dataXr, x).T[0]\n",
    "    dataYr = dataYr.reshape(-1)\n",
    "\n",
    "    # Find a misclassified index\n",
    "    for i, pred in enumerate(predictions):\n",
    "        # Check misclassification condition: prediction is incorrect if sign doesn't match the intended class\n",
    "        if pred >= 0:\n",
    "            misclassified_index = i\n",
    "            break\n",
    "    \n",
    "    # Retrieve the misclassified image and label\n",
    "\n",
    "    misclassified_image = dataXr[misclassified_index, :784].reshape(28, 28)  # Remove the augmented pixel\n",
    "    actual_label = dataYr[misclassified_index]\n",
    "    error_label = class1 if actual_label == class2 else class2\n",
    "\n",
    "    # Create a PIL image for adding text label\n",
    "    image = Image.fromarray((misclassified_image * 255).astype(np.uint8))  # Scale to 0-255 for display\n",
    "    image = image.convert(\"L\")  # Convert to grayscale\n",
    "    label_text = f\"Actual: {actual_labels[actual_label]}, Misclassified as: {actual_labels[error_label]}\"\n",
    "\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(label_text, pad=10)  # `pad` to add space between the image and title\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector-Based Evaluation and SGD Termination\n",
    "\n",
    "This section introduces a variation of SGD called **`sgd_mlogistic_svs`**, which integrates\n",
    "**early stopping using support vector (SVS) performance**. A separate subset of 500 support vectors\n",
    "(250 from each class) is sampled during preprocessing.\n",
    "\n",
    "During training, after every epoch (i.e., once the number of updates equals the size of SVS set),\n",
    "the model is evaluated on the SVS data. If the number of misclassified SVS points **does not improve**,\n",
    "training stops early. This is a lightweight way to regularize without relying on validation loss.\n",
    "\n",
    "**`driver_q5`** runs this SVS-aware SGD process multiple times and collects:\n",
    "- Train/test error rates\n",
    "- Iteration counts until early stopping\n",
    "\n",
    "Finally, results are printed for all defined **step size functions**, including fixed, diminishing,\n",
    "and Polyak-style schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_mlogistic_svs(trainXr, gamma, stepsizefunc, svsX, maxit=1e6):\n",
    "\n",
    "    #initialize weights\n",
    "    x = np.zeros((trainXr.shape[1], 1))\n",
    "    # random.seed(42)\n",
    "    x_list = []\n",
    "    s = svsX.shape[0]\n",
    "    j = 1\n",
    "    # x_avg = []\n",
    "    misclassified_list = []\n",
    "    k = 0\n",
    "    while k<maxit:\n",
    "\n",
    "        if k/j == s:\n",
    "            #sth iteration\n",
    "            x_avg = np.mean(np.array(x_list), axis=0)\n",
    "            _, misclassified = calculate_error(svsX, x_avg)\n",
    "            misclassified_list.append(len(misclassified))\n",
    "            if j>1:\n",
    "                if misclassified_list[j-1] >= misclassified_list[j-2]:\n",
    "                    print(\"SGD TERMINATION REACHED AT: \", k)\n",
    "                    return np.mean(np.array(x_list), axis=0), k\n",
    "\n",
    "            j += 1\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # choose random index\n",
    "        i = random.randint(0, trainXr.shape[0]-1)\n",
    "        ai = trainXr[i, :].reshape(-1,1)\n",
    "        alpha = stepsizefunc(ai, x, k)\n",
    "        grad = gradient_f(x, ai, gamma)\n",
    "        x = x - alpha*grad\n",
    "        x_list.append(x)\n",
    "        k += 1\n",
    "        \n",
    "\n",
    "    return np.mean(np.array(x_list), axis=0), maxit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver_q5(ss_func, maxit, class1, class2, gamma, actual_labels, display=False):\n",
    "    [x_train, y_train, x_test, y_test, svsX, svsY] = prepare_fashion_mnist(class1, class2)\n",
    "    # svsX, svsY, x_test, y_test = obtain_svsX(x_test, y_test, class1, class2)\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    iters = []\n",
    "    for i in range(3):\n",
    "\n",
    "      x, k = sgd_mlogistic_svs(x_train, gamma, ss_func, svsX, maxit)\n",
    "      iters.append(k)\n",
    "      train_error, _ = calculate_error(x_train, x)\n",
    "      # print(\"TRAIN ERROR: \", train_error)\n",
    "      train_errors.append(train_error)\n",
    "      #test error\n",
    "      test_error, _ = calculate_error(x_test, x)\n",
    "      # print(\"TEST ERROR: \", test_error)\n",
    "      test_errors.append(test_error)\n",
    "\n",
    "    min_train_error = min(train_errors)\n",
    "    max_train_error = max(train_errors)\n",
    "    mean_train_error = np.mean(train_errors)\n",
    "    min_test_error = min(test_errors)\n",
    "    max_test_error = max(test_errors)\n",
    "    mean_test_error = np.mean(test_errors)\n",
    "    print(f\"Min Train Error: {min_train_error}\")\n",
    "    print(f\"Max Train Error: {max_train_error}\")\n",
    "    print(f\"Mean Train Error: {mean_train_error}\")\n",
    "    print(f\"Min Test Error: {min_test_error}\")\n",
    "    print(f\"Max Test Error: {max_test_error}\")\n",
    "    print(f\"Mean Test Error: {mean_test_error}\")\n",
    "    min_iters = min(iters)\n",
    "    max_iters = max(iters)\n",
    "    mean_iters = np.mean(iters)\n",
    "    print(f\"Min Iterations: {min_iters}\")\n",
    "    print(f\"Max Iterations: {max_iters}\")\n",
    "    print(f\"Mean Iterations: {mean_iters}\")\n",
    "    if display:\n",
    "      display_misclassified_image(x_test, y_test, x, actual_labels, class1=class1, class2=class2)\n",
    "    return min_train_error, max_train_error, mean_train_error, min_test_error, max_test_error, mean_test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Pair: Ankle-Boot vs Sneaker\n",
      "Step Function: 1\n",
      "SGD TERMINATION REACHED AT:  1500\n",
      "SGD TERMINATION REACHED AT:  1500\n",
      "SGD TERMINATION REACHED AT:  1500\n",
      "Min Train Error: 0.0705\n",
      "Max Train Error: 0.07383333333333333\n",
      "Mean Train Error: 0.07236111111111111\n",
      "Min Test Error: 0.06666666666666667\n",
      "Max Test Error: 0.07066666666666667\n",
      "Mean Test Error: 0.06866666666666667\n",
      "Min Iterations: 1500\n",
      "Max Iterations: 1500\n",
      "Mean Iterations: 1500.0\n",
      "\n",
      "\n",
      "Step Function: 2\n",
      "SGD TERMINATION REACHED AT:  3000\n",
      "SGD TERMINATION REACHED AT:  2500\n",
      "SGD TERMINATION REACHED AT:  2500\n",
      "Min Train Error: 0.05575\n",
      "Max Train Error: 0.058666666666666666\n",
      "Mean Train Error: 0.05697222222222222\n",
      "Min Test Error: 0.056666666666666664\n",
      "Max Test Error: 0.058\n",
      "Mean Test Error: 0.057555555555555554\n",
      "Min Iterations: 2500\n",
      "Max Iterations: 3000\n",
      "Mean Iterations: 2666.6666666666665\n",
      "\n",
      "\n",
      "Step Function: 3\n",
      "SGD TERMINATION REACHED AT:  2500\n",
      "SGD TERMINATION REACHED AT:  2500\n",
      "SGD TERMINATION REACHED AT:  1500\n",
      "Min Train Error: 0.07733333333333334\n",
      "Max Train Error: 0.0865\n",
      "Mean Train Error: 0.08075\n",
      "Min Test Error: 0.07133333333333333\n",
      "Max Test Error: 0.08066666666666666\n",
      "Mean Test Error: 0.07466666666666667\n",
      "Min Iterations: 1500\n",
      "Max Iterations: 2500\n",
      "Mean Iterations: 2166.6666666666665\n",
      "\n",
      "\n",
      "Step Function: 4\n",
      "SGD TERMINATION REACHED AT:  1500\n",
      "SGD TERMINATION REACHED AT:  1000\n",
      "SGD TERMINATION REACHED AT:  1500\n",
      "Min Train Error: 0.08775\n",
      "Max Train Error: 0.08941666666666667\n",
      "Mean Train Error: 0.08866666666666667\n",
      "Min Test Error: 0.08066666666666666\n",
      "Max Test Error: 0.08333333333333333\n",
      "Mean Test Error: 0.082\n",
      "Min Iterations: 1000\n",
      "Max Iterations: 1500\n",
      "Mean Iterations: 1333.3333333333333\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma = 2.0\n",
    "gamma_b = 1e4\n",
    "stepsizefunc_1 = lambda x, y, z: phi_1(x,y,z, gamma)\n",
    "stepsizefunc_2 = lambda x, y, z: phi_2(x,y,z, gamma)\n",
    "stepsizefunc_3 = lambda x, y, z: phi_3(x,y,z, gamma)\n",
    "stepsizefunc_4 = lambda x, y, z: stoch_polyak(x,y, gamma, gamma_b)\n",
    "step_funcs = [stepsizefunc_1, stepsizefunc_2, stepsizefunc_3, stepsizefunc_4]\n",
    "class_pairs = [[9,7]]\n",
    "actual_labels = {9: \"Ankle-Boot\", 0: \"T-shirt\", 7:\"Sneaker\"}\n",
    "# numits = [50, 5000]\n",
    "maxit = 1e6\n",
    "num_runs = 0\n",
    "disp = False\n",
    "for class_pair in class_pairs:\n",
    "    \n",
    "    print(f\"Class Pair: {actual_labels[class_pair[0]]} vs {actual_labels[class_pair[1]]}\")\n",
    "    for f, step_func in enumerate(step_funcs):\n",
    "        print(f\"Step Function: {f+1}\")\n",
    "        min_train_error, max_train_error, mean_train_error, min_test_error, max_test_error, mean_test_error = driver_q5(step_func, maxit, class_pair[0], class_pair[1], gamma, actual_labels, display=disp)\n",
    "        print(\"\\n\")\n",
    "        num_runs += 1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
