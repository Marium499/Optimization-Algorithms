{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Classification with SGD and Adaptive Step Sizes\n",
    "\n",
    "This notebook demonstrates **binary classification** using **Stochastic Gradient Descent (SGD)**\n",
    "on the Fashion MNIST dataset. The focus is on experimenting with different **adaptive step size policies**\n",
    "to evaluate their effect on convergence and classification accuracy.\n",
    "\n",
    "Key features include:\n",
    "- Custom logistic loss and gradient implementation\n",
    "- Data preparation for binary classification (two selected classes)\n",
    "- SGD with various learning rate strategies: constant, diminishing, and Polyak-style\n",
    "- Misclassification error evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and SGD Optimization\n",
    "\n",
    "This section includes:\n",
    "- `prepare_fashion_mnist`: loads the Fashion MNIST dataset, filters two selected classes, rescales,\n",
    "adds an offset/bias term, and applies a sign flip for binary logistic regression.\n",
    "- `sgd_mlogistic`: runs stochastic gradient descent with a user-defined step size function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_fashion_mnist(class1, class2) -> x_train, y_train, x_test, y_test\n",
    "# class1, class2: 0~9\n",
    "# read fashion mnist from file\n",
    "# get trainX, trainY, testX and testY from file that corresponds to class1 and class2\n",
    "# find q = average value of all pixels across all images in trainX\n",
    "# append q to every image in trainX and testX as the 785th pixel/element - offset term - serves as regularization and bias\n",
    "# flip sign of class1 in trainY and testY\n",
    "# return x_train, y_train, x_test, y_test\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def prepare_fashion_mnist(class1, class2):\n",
    "\n",
    "    # load data\n",
    "    data = loadmat('fashion_mnist.mat')\n",
    "    trainX = data['trainX_uint8']\n",
    "    trainY = data['trainY_uint8']\n",
    "    testX = data['testX_uint8']\n",
    "    testY = data['testY_uint8']\n",
    "\n",
    "    # filter out class1 and class2\n",
    "\n",
    "    train_indices = np.where((trainY == class1) | (trainY == class2))\n",
    "    trainX = trainX[:,train_indices[0]]\n",
    "    trainY = trainY[train_indices[0]]\n",
    "\n",
    "    test_indices = np.where((testY == class1) | (testY == class2))\n",
    "    testX = testX[:,test_indices[0]]\n",
    "    testY = testY[test_indices[0]]\n",
    "\n",
    "    #convert to float\n",
    "    trainX = trainX.astype(float)\n",
    "    testX = testX.astype(float)\n",
    "\n",
    "    # find q\n",
    "    q = np.mean(trainX)\n",
    "\n",
    "    # append q to every image in trainX and testX as the 785th pixel/element\n",
    "    trainX = np.append(trainX, q * np.ones((1, trainX.shape[1])), axis=0)\n",
    "    testX = np.append(testX, q * np.ones((1, testX.shape[1])), axis=0)\n",
    "\n",
    "    # flip sign of class1 in trainX and testX\n",
    "    class1_indices = np.where((trainY == class1))\n",
    "    trainX[:,class1_indices[0]] = -trainX[:,class1_indices[0]]\n",
    "\n",
    "    class1_indices = np.where((testY == class1))\n",
    "    testX[:,class1_indices[0]] = -testX[:,class1_indices[0]]\n",
    "\n",
    "    return trainX.T, trainY.T, testX.T, testY.T\n",
    "\n",
    "\n",
    "def sgd_mlogistic(trainXr, gamma, stepsizefunc, numit):\n",
    "\n",
    "    #initialize weights\n",
    "    x = np.zeros((trainXr.shape[1], 1))\n",
    "    # random.seed(42)\n",
    "    x_list = []\n",
    "    for k in range(numit):\n",
    "        \n",
    "        # choose random index\n",
    "        i = random.randint(0, trainXr.shape[0]-1)\n",
    "        ai = trainXr[i, :].reshape(-1,1)\n",
    "        alpha = stepsizefunc(ai, x, k)\n",
    "        grad = gradient_f(x, ai, gamma)\n",
    "        x = x - alpha*grad\n",
    "        x_list.append(x)\n",
    "\n",
    "    return np.mean(x_list, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "This section defines:\n",
    "- Logistic loss (`f_x`) and its gradient (`gradient_f`)\n",
    "- Step size policies including diminishing steps and Polyak step sizes (`stoch_polyak`)\n",
    "- Supporting math utilities including Newton's method and psi/phi functions\n",
    "- Misclassification error computation for evaluating model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER FUNCTIONS\n",
    "import math\n",
    "\n",
    "def P(c):\n",
    "    return lambda t: 1 - 4 * t + 4 * (t**2) - 4 * c * (t**3) + 4 * c * (t**4)\n",
    "\n",
    "\n",
    "def P_prime(c):\n",
    "    return lambda t: -4 + 8 * t - 12 * c * (t**2) + 16 * c * (t**3)\n",
    "\n",
    "\n",
    "def newton_method_on_P(c):\n",
    "    initial_guess = min(0.5, (4 * c) ** (-1 / 3))\n",
    "    last_x = initial_guess\n",
    "\n",
    "    for i in range(20):\n",
    "        x = last_x - P(c)(last_x) / P_prime(c)(last_x)\n",
    "        last_x = x\n",
    "    return last_x\n",
    "\n",
    "def gradient_f(x, a, gamma):\n",
    "    # Compute the gradient of f with respect to x for a single data point a\n",
    "    gradient = (0.5 + 0.5 * np.dot(a.T, x) / np.sqrt(1 + np.dot(a.T, x)**2)) * a\n",
    "    gradient += gamma * x  # Add regularization term\n",
    "    return gradient\n",
    "\n",
    "def f_x(x, a, gamma):\n",
    "    reg_term = (gamma / 2) * np.linalg.norm(x)**2\n",
    "\n",
    "    t = np.dot(a.T, x)\n",
    "    psi_t = psi(t)\n",
    "    return psi_t + reg_term\n",
    "\n",
    "\n",
    "def find_min_of_f(a_norm_squared, gamma):\n",
    "    c = (a_norm_squared**2) / (gamma**2)\n",
    "    t_star = newton_method_on_P(c)\n",
    "    theta_star = t_star / gamma\n",
    "    psi = lambda t: t / 2 + math.sqrt(1 + t**2) / 2\n",
    "    return theta_star, psi(-a_norm_squared * theta_star) + (\n",
    "        gamma / 2\n",
    "    ) * a_norm_squared * (theta_star**2)\n",
    "\n",
    "def psi(t):\n",
    "    return t / 2 + np.sqrt(1 + t**2) / 2\n",
    "\n",
    "def phi_1(a, x, k, gamma):\n",
    "    L = 4*gamma**2\n",
    "\n",
    "    return (gamma*1e-2)/L\n",
    "\n",
    "def phi_2(a, x, k, gamma):\n",
    "    L = 4*gamma**2\n",
    "\n",
    "    return (gamma*1e-5)/L\n",
    "\n",
    "def phi_3(a, x, k, gamma):\n",
    "    L = 4*gamma**2\n",
    "    m = gamma\n",
    "    c = L/(m**2)\n",
    "\n",
    "    return 1/(m*(c + 0.75*k))\n",
    "\n",
    "def stoch_polyak(a, x, gamma, gammab=1e4):\n",
    "    \n",
    "    _, f_min = find_min_of_f(np.linalg.norm(a)**2, gamma)\n",
    "    f_current = f_x(x, a, gamma)\n",
    "    squared_norm_grad = np.linalg.norm(gradient_f(x, a, gamma))**2\n",
    "    alpha = min(gammab, 2 * (f_current - f_min) / squared_norm_grad)\n",
    "    # print(alpha)\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "def calculate_error(dataXr, x):\n",
    "    predictions = np.dot(dataXr, x)\n",
    "    # Determine misclassified points (nonnegative means misclassified)\n",
    "    misclassified = predictions[predictions >= 0]\n",
    "    error_rate = len(misclassified)/len(predictions)\n",
    "\n",
    "    return error_rate, predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def display_misclassified_image(dataXr, dataYr, x,actual_labels, class1, class2):\n",
    "    \"\"\"\n",
    "    Display a misclassified image with a label showing the actual and misclassified classes.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataXr: The test dataset (or train dataset), including the augmented pixel for offset.\n",
    "    - dataYr: The labels of the test dataset.\n",
    "    - x: The model weights from sgd_mlogistic.\n",
    "    - actual_labels: Original labels for class1 and class2 (e.g., 0 for T-shirt, 9 for ankle-boot).\n",
    "    - class1, class2: The integer class labels (for binary classification).\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    predictions = np.dot(dataXr, x).T[0]\n",
    "    dataYr = dataYr.reshape(-1)\n",
    "\n",
    "    # Find a misclassified index\n",
    "    for i, pred in enumerate(predictions):\n",
    "        # Check misclassification condition: prediction is incorrect if sign doesn't match the intended class\n",
    "        if pred >= 0:\n",
    "            misclassified_index = i\n",
    "            break\n",
    "    \n",
    "    # Retrieve the misclassified image and label\n",
    "\n",
    "    misclassified_image = dataXr[misclassified_index, :784].reshape(28, 28)  # Remove the augmented pixel\n",
    "    actual_label = dataYr[misclassified_index]\n",
    "    error_label = class1 if actual_label == class2 else class2\n",
    "\n",
    "    # Create a PIL image for adding text label\n",
    "    image = Image.fromarray((misclassified_image * 255).astype(np.uint8))  # Scale to 0-255 for display\n",
    "    image = image.convert(\"L\")  # Convert to grayscale\n",
    "    label_text = f\"Actual: {actual_labels[actual_label]}, Misclassified as: {actual_labels[error_label]}\"\n",
    "\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(label_text, pad=10)  # `pad` to add space between the image and title\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver(ss_func, numit, class1, class2, gamma, actual_labels, display=False):\n",
    "    [x_train, y_train, x_test, y_test] = prepare_fashion_mnist(class1, class2)\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for i in range(3):\n",
    "\n",
    "      x = sgd_mlogistic(x_train, gamma, ss_func, numit)\n",
    "      train_error, _ = calculate_error(x_train, x)\n",
    "      # print(\"TRAIN ERROR: \", train_error)\n",
    "      train_errors.append(train_error)\n",
    "      #test error\n",
    "      test_error, test_pred = calculate_error(x_test, x)\n",
    "      # print(\"TEST ERROR: \", test_error)\n",
    "      test_errors.append(test_error)\n",
    "\n",
    "    min_train_error = min(train_errors)\n",
    "    max_train_error = max(train_errors)\n",
    "    mean_train_error = np.mean(train_errors)\n",
    "    min_test_error = min(test_errors)\n",
    "    max_test_error = max(test_errors)\n",
    "    mean_test_error = np.mean(test_errors)\n",
    "    print(f\"Min Train Error: {min_train_error}\")\n",
    "    print(f\"Max Train Error: {max_train_error}\")\n",
    "    print(f\"Mean Train Error: {mean_train_error}\")\n",
    "    print(f\"Min Test Error: {min_test_error}\")\n",
    "    print(f\"Max Test Error: {max_test_error}\")\n",
    "    print(f\"Mean Test Error: {mean_test_error}\")\n",
    "    if display:\n",
    "      display_misclassified_image(x_test, y_test, x, actual_labels, class1=class1, class2=class2)\n",
    "    return min_train_error, max_train_error, mean_train_error, min_test_error, max_test_error, mean_test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Pair: Ankle-Boot vs T-shirt\n",
      "Number of Iterations: 50\n",
      "Step Function: 1\n",
      "Min Train Error: 0.006666666666666667\n",
      "Max Train Error: 0.023916666666666666\n",
      "Mean Train Error: 0.014666666666666666\n",
      "Min Test Error: 0.008\n",
      "Max Test Error: 0.026\n",
      "Mean Test Error: 0.016\n",
      "\n",
      "\n",
      "Number of Iterations: 5000\n",
      "Step Function: 1\n",
      "Min Train Error: 0.00125\n",
      "Max Train Error: 0.0015\n",
      "Mean Train Error: 0.0013611111111111113\n",
      "Min Test Error: 0.001\n",
      "Max Test Error: 0.0025\n",
      "Mean Test Error: 0.0016666666666666668\n",
      "\n",
      "\n",
      "Number of Iterations: 50\n",
      "Step Function: 2\n",
      "Min Train Error: 0.0025\n",
      "Max Train Error: 0.011333333333333334\n",
      "Mean Train Error: 0.006916666666666667\n",
      "Min Test Error: 0.002\n",
      "Max Test Error: 0.011\n",
      "Mean Test Error: 0.007333333333333333\n",
      "\n",
      "\n",
      "Number of Iterations: 5000\n",
      "Step Function: 2\n",
      "Min Train Error: 0.0009166666666666666\n",
      "Max Train Error: 0.0011666666666666668\n",
      "Mean Train Error: 0.0010277777777777778\n",
      "Min Test Error: 0.0005\n",
      "Max Test Error: 0.001\n",
      "Mean Test Error: 0.0008333333333333334\n",
      "\n",
      "\n",
      "Number of Iterations: 50\n",
      "Step Function: 3\n",
      "Min Train Error: 0.00475\n",
      "Max Train Error: 0.015833333333333335\n",
      "Mean Train Error: 0.008611111111111111\n",
      "Min Test Error: 0.008\n",
      "Max Test Error: 0.0185\n",
      "Mean Test Error: 0.011666666666666667\n",
      "\n",
      "\n",
      "Number of Iterations: 5000\n",
      "Step Function: 3\n",
      "Min Train Error: 0.0014166666666666668\n",
      "Max Train Error: 0.003416666666666667\n",
      "Mean Train Error: 0.002166666666666667\n",
      "Min Test Error: 0.001\n",
      "Max Test Error: 0.004\n",
      "Mean Test Error: 0.0023333333333333335\n",
      "\n",
      "\n",
      "Number of Iterations: 50\n",
      "Step Function: 4\n",
      "Min Train Error: 0.004666666666666667\n",
      "Max Train Error: 0.008666666666666666\n",
      "Mean Train Error: 0.006527777777777778\n",
      "Min Test Error: 0.005\n",
      "Max Test Error: 0.009\n",
      "Mean Test Error: 0.0075\n",
      "\n",
      "\n",
      "Number of Iterations: 5000\n",
      "Step Function: 4\n",
      "Min Train Error: 0.006333333333333333\n",
      "Max Train Error: 0.007916666666666667\n",
      "Mean Train Error: 0.0071666666666666675\n",
      "Min Test Error: 0.0075\n",
      "Max Test Error: 0.008\n",
      "Mean Test Error: 0.007833333333333333\n",
      "\n",
      "\n",
      "Class Pair: Ankle-Boot vs Sneaker\n",
      "Number of Iterations: 50\n",
      "Step Function: 1\n",
      "Min Train Error: 0.094\n",
      "Max Train Error: 0.15525\n",
      "Mean Train Error: 0.11763888888888889\n",
      "Min Test Error: 0.093\n",
      "Max Test Error: 0.156\n",
      "Mean Test Error: 0.11883333333333335\n",
      "\n",
      "\n",
      "Number of Iterations: 5000\n",
      "Step Function: 1\n",
      "Min Train Error: 0.07258333333333333\n",
      "Max Train Error: 0.0745\n",
      "Mean Train Error: 0.07327777777777778\n",
      "Min Test Error: 0.0715\n",
      "Max Test Error: 0.0755\n",
      "Mean Test Error: 0.07333333333333332\n",
      "\n",
      "\n",
      "Number of Iterations: 50\n",
      "Step Function: 2\n",
      "Min Train Error: 0.092\n",
      "Max Train Error: 0.11516666666666667\n",
      "Mean Train Error: 0.10208333333333335\n",
      "Min Test Error: 0.092\n",
      "Max Test Error: 0.107\n",
      "Mean Test Error: 0.09849999999999999\n",
      "\n",
      "\n",
      "Number of Iterations: 5000\n",
      "Step Function: 2\n",
      "Min Train Error: 0.04783333333333333\n",
      "Max Train Error: 0.0515\n",
      "Mean Train Error: 0.050083333333333334\n",
      "Min Test Error: 0.043\n",
      "Max Test Error: 0.052\n",
      "Mean Test Error: 0.048666666666666664\n",
      "\n",
      "\n",
      "Number of Iterations: 50\n",
      "Step Function: 3\n",
      "Min Train Error: 0.11525\n",
      "Max Train Error: 0.13975\n",
      "Mean Train Error: 0.1284722222222222\n",
      "Min Test Error: 0.1145\n",
      "Max Test Error: 0.1465\n",
      "Mean Test Error: 0.13066666666666668\n",
      "\n",
      "\n",
      "Number of Iterations: 5000\n",
      "Step Function: 3\n",
      "Min Train Error: 0.07058333333333333\n",
      "Max Train Error: 0.07116666666666667\n",
      "Mean Train Error: 0.07080555555555555\n",
      "Min Test Error: 0.0705\n",
      "Max Test Error: 0.073\n",
      "Mean Test Error: 0.07166666666666666\n",
      "\n",
      "\n",
      "Number of Iterations: 50\n",
      "Step Function: 4\n",
      "Min Train Error: 0.093\n",
      "Max Train Error: 0.12491666666666666\n",
      "Mean Train Error: 0.10580555555555556\n",
      "Min Test Error: 0.092\n",
      "Max Test Error: 0.1155\n",
      "Mean Test Error: 0.1025\n",
      "\n",
      "\n",
      "Number of Iterations: 5000\n",
      "Step Function: 4\n",
      "Min Train Error: 0.08775\n",
      "Max Train Error: 0.09016666666666667\n",
      "Mean Train Error: 0.08913888888888888\n",
      "Min Test Error: 0.086\n",
      "Max Test Error: 0.0885\n",
      "Mean Test Error: 0.08733333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma = 2.0\n",
    "gamma_b = 1e4\n",
    "stepsizefunc_1 = lambda x, y, z: phi_1(x,y,z, gamma)\n",
    "stepsizefunc_2 = lambda x, y, z: phi_2(x,y,z, gamma)\n",
    "stepsizefunc_3 = lambda x, y, z: phi_3(x,y,z, gamma)\n",
    "stepsizefunc_4 = lambda x, y, z: stoch_polyak(x,y, gamma, gamma_b)\n",
    "step_funcs = [stepsizefunc_1, stepsizefunc_2, stepsizefunc_3, stepsizefunc_4]\n",
    "class_pairs = [[9,0], [9,7]]\n",
    "actual_labels = {9: \"Ankle-Boot\", 0: \"T-shirt\", 7:\"Sneaker\"}\n",
    "numits = [50, 5000]\n",
    "num_runs = 0\n",
    "for class_pair in class_pairs:\n",
    "    \n",
    "    print(f\"Class Pair: {actual_labels[class_pair[0]]} vs {actual_labels[class_pair[1]]}\")\n",
    "    for f, step_func in enumerate(step_funcs):\n",
    "        for numit in numits:\n",
    "            if num_runs == 15:\n",
    "                disp = True\n",
    "            else:\n",
    "                disp = False\n",
    "            print(f\"Number of Iterations: {numit}\")\n",
    "            print(f\"Step Function: {f+1}\")\n",
    "            min_train_error, max_train_error, mean_train_error, min_test_error, max_test_error, mean_test_error = driver(step_func, numit, class_pair[0], class_pair[1], gamma, actual_labels, display=disp)\n",
    "            print(\"\\n\")\n",
    "            num_runs += 1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
